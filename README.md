# DEVOPS Challenge

1. [OK] Create an EKS cluster in a secure, scalable and efficient way
2. [OK] Use Helm Charts to install and configure a Grafana on the Kubernetes cluster.
3. [OK] Create an AWS Timestream or Athena and add test data to it.
4. [NOK] Add the AWS Timestream or Athena datasource to Grafana and display the Timestream or Athena data via a dashboard. You should create graphs, tables, and dashboards that present the information clearly and intuitively.
5. [OK] Provide observability of the resources created during the challenge. You should monitor the performance, availability, and consumption of the resources.
6. [OK]Add a microservice of your choice to the cluster, monitor its logs and resource usage in Grafana, and implement a CI/CD pipeline for this service.

# Devcontainer

This repository uses devcontainer to make your life easier :). Install the devcontainer plugin in your VSCode and be happy.

The dev container is already configured with the following features

- Terraform
- AWS-CLI
- kubectl-helm-minikube
- eksctl
- node
- docker-outside-of-docker

Reference: [DevContainer](https://containers.dev/)

# Estrutura de pastas

# Folder structure

```
├── README.md
├── .aws -> Configure credentials in the devcontainer
├── .devcontainer -> Set up a container with all the resources needed to run the project
├── .github -> Configures the GHA responsible for the project's CI/CD
├── sample-service -> Configures an app responsible for sending 'mocked' information to AWS Timestream
| ├── src -> App in nodejs
| └── terraform -> Configure the resources needed for the sample-service
| | ├── build-image.tf -> Command to build the image, push it to the ecr, and deploy the app on k8s
| │ ├── main.tf -> Configure providers, backend and local variables 
| │ ├── outputs.tf -> Configure outputs generated by the resources created
| │ ├── timestream.tf -> Configure a database and table in AWS timestream
| │ └── variables.tf -> Declaration of project variables
└── terraform -> Configure the resources needed for this project
  ├── k8s -> Configure k8s
  │ ├── dev.tfvars -> Configure the variables for the development environment
  │ ├── ecr.tf -> Configures the registry container
  │ ├── eks.tf -> Configure the Kubernetes cluster
  │ ├── main.tf -> Configure providers, backend and local variables 
  │ ├── network.tf -> Configure network resources such as vpc, subnets,
  │ ├── outputs.tf -> Configure outputs generated by the resources created
  │ └── variables.tf -> Declaration of project variables
  └── k8s-observability -> Configure k8s-observability
    ├── eks_observability.tf -> Configure Prometheus Stack (Prometheus + Grafana)
    ├── main.tf -> Configure providers, backend and local variables 
    └── variables.tf -> Declaration of project variables
    
# Setup

To run terraform you need to have a previously created bucket where the state files will be saved. Note that s3 paths are global, so if you want to run this repository, you'll need to change this path in the Terraform backend.

If you want to create a bucket, you can use the following commands 

```bash 
aws s3api create-bucket --bucket <BucketName>
aws iam create-policy \
    --policy-name TFStatePolicy \
    --policy-document \
'{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "s3:ListBucket",
      "Resource": "arn:aws:s3:::<BucketName>.tfstate"
    },
    {
      "Effect": "Allow",
      "Action": ["s3:GetObject", "s3:PutObject"],
      "Resource": "arn:aws:s3:::<BucketName>.tfstate/terraform.tfstate"
    }
  ]
}'
```

## Deploy infrastructure

### Manual Deploy/Destroy
First configure the credentials file inside the .aws folder with your credentials

Inside the devcontainer, run the following commands:

```bash 
cd terraform/k8s

terraform init

terraform plan --var-file=dev.tfvars

terraform apply --var-file=dev.tfvars

cd terraform/k8s-observability

terraform init

terraform plan

terraform apply -target=null_resource.kubectl # TODO: FIX ME

terraform apply
```

The division into two deployments is necessary because in order to create some obsevability resources it is necessary to have k8s already created. There is room for improvement, I just need to think about it carefully.

If you want to destroy the resources, run the following:
   
```bash 
cd terraform/k8s

terraform init

terraform destroy --var-file=dev.tfvars

cd terraform/k8s-observability

terraform init

terraform destroy
```

### CI/CD

This project has some actions for creating resources

Remember to configure the following variables in `Actions secrets and variables`

```
AWS_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY
```

It is also necessary to enter the `Workflow permissions` configuration and change the permission to `Read and write permissions`

#### cicd-deploy-k8s-complete
This action is triggered when code is 'merged' into `main` or when a PR is opened with target `main`. This action could be improved to deploy to several environments, but as the aim of this project is to create a single environment, this feature has not been implemented.

When triggered by a PR event, the action validates the style, the code, creates the execution plan and outputs the result as a comment in the PR

When triggered by a merge in the `main`, the action validates the style, the code, creates the execution plan and applies the execution plan.

#### manual-deploy-k8s
This action is fired manually and is used to create or destroy the infrastructure manually

# Sample service 

This is a small test service that deploys an AWS Timestream and generates records in the AWS Timestream. 

To run it locally, execute the following commands

```
cd sample-service/src

npm install

node main.js
```

This service will write the record every second to AWS Timestream.

### Manual Deploy/Destroy
First, configure the credentials file inside the .aws folder with your credentials

Inside the devcontainer, run the following commands:

```bash 
cd sample-service/terraform

aws eks update-kubeconfig --region us-east-1 --name challange-eks 

terraform init

terraform plan -var "AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID" -var "AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY"

terraform apply -var "AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID" -var "AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY"

PATCH='{"spec":{"template":{"metadata":{"annotations":{"timestamp":"'$(date)'"}}}}}'; kubectl patch deployment sample-service -p "$PATCH"
```
   
```bash 
cd sample-service/terraform

aws eks update-kubeconfig --region us-east-1 --name challange-eks 

terraform init

terraform destroy -var "AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID" -var "AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY"
```

### CI/CD

This project has some actions for creating resources

Remember to configure the following variables in `Actions secrets and variables`

```
AWS_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY
```

It is also necessary to enter the `Workflow permissions` configuration and change the permission to `Read and write permissions`.

#### cicd-sample-service
This action is triggered when code is merged into `main` or when a PR is opened targeting `main`. This action could be improved to deploy to several environments, but as the aim of this project is to create a single environment, this feature has not been implemented.

When triggered by a PR event, the action validates the style, the code, creates the execution plan and outputs the result as a comment in the PR

When triggered by a merge in `main`, the action validates the style, the code, creates the execution plan and applies the execution plan.

#### manual-sample-service
This action is fired manually and is used to create or destroy the infrastructure manually


# Grafana

1. Make sure that kubeconfig is up to date, if not run the following command `aws eks --region "us-east-1" update-kubeconfig --name challange-eks`.

2. Port-foward grafana to your machine by running the following command `kubectl port-forward --namespace monitoring service/kube-prometheus-stack-grafana 3000:80`.

3. Make sure that the devcontainer is also port-fowarding to your machine, if not do it manually

4. To recover the login access run the following command `kubectl get secret --namespace monitoring kube-prometheus-stack-grafana -o jsonpath="{.data.admin-user}" | base64 --decode`

5. To recover your password, run the following command `kubectl get secret --namespace monitoring kube-prometheus-stack-grafana -o jsonpath="{.data.admin-password}" | base64 --decode`.

Now just go to `localhost:3000` and you'll be able to access grafana

# Possible improvements 

## Terraform 
- There are some values that should be variables because they can change according to the environment.
- The statefile is not being encrypted, versioned or saving the statelock in dynamoDB
- Configure an IAM so that grafana can access AWS Timestream

## GHA

- The pipelines are configured to deploy to only one environment
- There is a dependency between the simple-service pipeline and the k8s pipeline, but they are starting at the same time. If k8s is not created, the simple-service pipeline will fail when trying to deploy to k8s.
- The simple-service pipeline could have lint checking, and test checking if a test was implemented, so it would be closer to what a productive pipeline would look like.

## DevContainer

- Add localstack-pro to be able to test the project locally (I tried but it started giving errors when creating the cluster, so I stopped due to lack of time)
